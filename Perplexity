from gensim.models import LdaModel, LdaMulticore
from gensim.test.utils import common_texts
from gensim.corpora.dictionary import Dictionary
import numpy as np
import gensim

'''
command line to compute perplexity:
topic-100
1) bin\mallet import-file --input ./data_wsj/train_label.txt --output ./data_wsj/train_label.mallet --keep-sequence True --remove-stopwords True
2) bin\mallet train-topics --input ./data_wsj/train_label.mallet --inferencer-filename ./data_wsj/infer.mallet --num-topics 100 --output-topic-keys ./data_wsj/topic-100-keys.txt --output-doc-topics ./data_wsj/topic-100-composition.txt --evaluator-filename ./data_wsj/eval_100.mallet --num-top-words 10
3) bin\mallet import-file --input ./data_wsj/test_label.txt --output ./data_wsj/test_label.mallet --keep-sequence True --remove-stopwords True --use-pipe-from ./data_wsj/train_label.mallet
4) bin\mallet evaluate-topics --evaluator ./data_wsj/eval_100.mallet --input ./data_wsj/test_label.mallet --output-doc-probs ./data_wsj/doc-probs-100.txt --output-prob ./data_wsj/prob-100.txt
5) bin\mallet run cc.mallet.util.DocumentLengths --input ./data_wsj/test_label.mallet > wordcount-100.txt
perplexity = np.exp(-(./data_wsj/prob-100.txt) / sum(wordcount-100.txt))
***********************************************************************This perplexity is much lower than the below method****************************************************************
'''

common_dictionary = Dictionary(common_texts)
common_corpus = [common_dictionary.doc2bow(text) for text in common_texts]

# Train the model on the corpus.
mallet_path = root_path + '/mallet-2.0.8/bin/mallet' # update this path
lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=common_corpus, num_topics=100, id2word=common_dictionary, prefix='./')

# copy the parameters in pretrained mellet_model to gensim LdaModel
def mallet_to_lda(mallet_model):
    model_gensim = LdaModel(
        id2word=mallet_model.id2word, num_topics=mallet_model.num_topics,
        alpha=mallet_model.alpha, iterations=1000,
        gamma_threshold=0.001,
        dtype=np.float32
    )
    model_gensim.sync_state()
    model_gensim.state.sstats = mallet_model.wordtopics
    return model_gensim

# copy parameters
lda = mallet_to_lda(lda_mallet)

# print the topic word lists
for p, list in lda.show_topics(num_topics=100, num_words=10, formatted=False):
    each_topic = []
    for w, score in list:
        each_topic.append(w)
    print(' '.join(each_topic))


# test_dictionary = Dictionary(common_texts)
test_corpus = [common_dictionary.doc2bow(text) for text in test_corpus]

perwordbound = lda.log_perplexity(test_corpus)
# Perplexity: a measure of how good the model is. lower the better.
print('\nperwordbound: ', perwordbound)  
print('\nPerplexity: ', np.exp2(-perwordbound))
